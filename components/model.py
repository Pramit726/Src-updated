# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z1St9udZSsyOi2ISCUiK7JngjiN0djka

# **Food Ingredient Image Classification with Transfer Learning (MobileNetV2)**

The notebook explores various models built  using transfer learning techniques.
"""

# Commented out IPython magic to ensure Python compatibility.
# %run -i Data_preprocessing.ipynb

"""### **Importing necessary libraries**"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Activation
from keras.activations import relu, softmax
from keras.optimizers import Adam
from keras.losses import SparseCategoricalCrossentropy
from keras.applications import MobileNetV2

"""### **Loading and Configuring MobileNetV2 Base Model**"""

# Load MobileNetV2 as the base model
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))

# Set the base model as non-trainable
base_model.trainable = False

"""### **Summary of MobileNetV2 Base Model**"""

base_model.summary()

"""### **Building a Feature Extraction Model**"""

def build_model_feature_extraction(hp):
    """
    Build a Sequential model for feature extraction using MobileNetV2 as the base model.

    Args:
    - hp (HyperParameters): An instance of HyperParameters class for tuning hyperparameters.

    Returns:
    - model (Sequential): A compiled Keras Sequential model for feature extraction.
    """
    # Create a Sequential model
    model = Sequential()

    # Add the base model to the Sequential model
    model.add(base_model)

    # Add the GlobalAveragePooling2D layer
    model.add(GlobalAveragePooling2D())

    # Add the Flatten layer
    model.add(Flatten())

    # Add the Dense and Activation layers
    model.add(Dense(units=512))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    model.add(Dense(units=256))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    model.add(Dense(units=128))
    model.add(Activation('relu'))

    model.add(Dense(units=len(data_cat)))
    model.add(Activation('softmax'))

    # Choose learning rate from a predefined list
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model

"""### **Configuring Fine-Tuning for Specific Layers**"""

def build_model_fine_tuning():
    """
    Build a fine-tuning model by allowing training for specific layers of the base model.

    Returns:
    - None
    """
    base_model.trainable = True

    set_trainable = False

    for layer in base_model.layers:
        if layer.name == 'block_6_expand':
            set_trainable = True
        if set_trainable:
            layer.trainable = True
        else:
            layer.trainable = False